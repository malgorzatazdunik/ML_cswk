{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hours\t0.50  |\t0.75  |\t1.00  |\t1.25  |\t1.50  |\t 1.75  |   1.75  |  2.00  | \t2.25 |\t2.50 |\t2.75 |\t3.00 |\t3.25 |\t3.50 |\t4.00 |\t4.25 |\t4.50 |\t4.75 |\t5.00 |\t5.50 |\n",
    "<br>\n",
    "Pass\t0 |\t0 |\t0 |\t0 |\t0 |\t0 |\t1 |\t0 |\t1 |\t0 |\t1 |0 |1 |\t0 |\t1 |\t1 |\t1 |\t1 |\t1 |\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to do: 1. Link tutorials. 2. comments, doc strings. 3. report 4. questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac <br>\n",
    "https://predictiveprogrammer.com/machine-learning-from-scratch-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.asarray ([[0.50], [0.75], [1.00], [1.25], [1.50], [1.75], [3.00], [4.75], [1.75], [3.25], [5.00], [2.00], [2.25], [2.50], [2.75], [3.50], [4.00], [4.25], [4.50], [5.50]]) \n",
    "y = np.asarray([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,0,1,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:15], y[:15], X[15:], y[15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis. <br>\n",
    "A function takes inputs and returns outputs. To generate probabilities, logistic regression uses a function that gives outputs between 0 and 1 for all values of X. There are many functions that meet this description, but the used in this case is the logistic function. From here we will refer to it as sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X,theta):\n",
    "    z = np.dot(X, theta[0]) + theta[1]\n",
    "    #print(z)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "weights = [np.random.rand(X.shape[1], 1),np.zeros((1,))]\n",
    "h = sigmoid(X,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.33021132]]), array([0.])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5411829 ],\n",
       "       [0.56160009],\n",
       "       [0.58181079],\n",
       "       [0.60175048],\n",
       "       [0.62135817],\n",
       "       [0.6405771 ],\n",
       "       [0.72921312],\n",
       "       [0.8275702 ],\n",
       "       [0.6405771 ],\n",
       "       [0.74520248],\n",
       "       [0.8390338 ],\n",
       "       [0.65935532],\n",
       "       [0.67764617],\n",
       "       [0.69540858],\n",
       "       [0.71260734],\n",
       "       [0.76055772],\n",
       "       [0.7893223 ],\n",
       "       [0.80272256],\n",
       "       [0.81546975],\n",
       "       [0.86010499]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function <br>\n",
    "Functions have parameters/weights (represented by theta in our notation) and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function, defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: understand loss function and fix it possibly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189412166591064"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent <br>\n",
    "Our goal is to minimize the loss function and the way we have to achive it is by increasing/decreasing the weights, i.e. fitting them. The question is, how do we know what parameters should be biggers and what parameters should be smallers? The answer is given by the derivative of the loss function with respect to each weight. It tells us how loss would change if we modified the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.dot(X.T, (h - y)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1054066,  2.1054066,  2.1054066,  2.1054066,  2.1054066,\n",
       "         2.1054066, -0.6820934,  2.1054066, -0.6820934,  2.1054066,\n",
       "        -0.6820934,  2.1054066, -0.6820934,  2.1054066, -0.6820934,\n",
       "        -0.6820934, -0.6820934, -0.6820934, -0.6820934, -0.6820934]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.33021132]]), array([0.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: understand calculating gradient -> shouldn't be the same as linear reg? see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(X,y,old_theta,alpha,treshold):\n",
    "    loss_list = []\n",
    "\n",
    "    theta = old_theta.copy()\n",
    "\n",
    "    loss = loss_(sigmoid(X,theta),y)\n",
    "    loss_update = 0\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    \n",
    "    while(abs(loss-loss_update)> treshold):\n",
    "    #while(index<50):\n",
    "            \n",
    "            h = sigmoid(X,theta)\n",
    "            \n",
    "            gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "            #print(gradient)\n",
    "            theta[0] -= alpha * gradient\n",
    "            #print('theta update', theta)\n",
    "            theta[1] -= alpha * np.mean(h-y)\n",
    "            \n",
    "            h = sigmoid(X,theta)\n",
    "            \n",
    "            loss = loss_update\n",
    "            loss_update=loss_(h,y)\n",
    "            loss_list.append(loss_update)\n",
    "            #print([loss,loss_update])\n",
    "            \n",
    "            index+=1\n",
    "\n",
    "\n",
    "    print('GD stopped at loss %s, with coefficients: %s' % (loss,theta), 'index', index)\n",
    "    return theta,loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD stopped at loss 0.5785958227001309, with coefficients: [array([0.39449148]), array([-1.48190368])] index 25\n"
     ]
    }
   ],
   "source": [
    "old_theta = [np.random.rand(X_train.shape[1]),np.zeros((1,))]\n",
    "learning_rate = 0.9\n",
    "loss_treshold = 0.001\n",
    "theta, loss = LogisticRegression(X_train,y_train,old_theta,learning_rate,loss_treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.39449148]), array([-1.48190368])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6967104699081853,\n",
       " 0.6565298310869803,\n",
       " 0.6438217054514442,\n",
       " 0.6346258407917317,\n",
       " 0.62789915870218,\n",
       " 0.6222570880932162,\n",
       " 0.6173790215467064,\n",
       " 0.6130384585255066,\n",
       " 0.6091319804301064,\n",
       " 0.6055915139627559,\n",
       " 0.6023707651223381,\n",
       " 0.5994343002589312,\n",
       " 0.5967528456646538,\n",
       " 0.5943014671716144,\n",
       " 0.5920582055277384,\n",
       " 0.5900036097909609,\n",
       " 0.588120254302949,\n",
       " 0.5863925140631182,\n",
       " 0.58480632189418,\n",
       " 0.583349007712038,\n",
       " 0.582009138587066,\n",
       " 0.5807763914204064,\n",
       " 0.5796414339123235,\n",
       " 0.5785958227001309,\n",
       " 0.5776319108977883]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigmoid(X_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47472568, 0.52399711, 0.54851813, 0.57280579, 0.66547723])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "plt.plot(y_test - 0.1,'g*')\n",
    "plt.plot(y_pred,'b.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_data = {'True':y_test, 'Predicted':y_pred}\n",
    "results = pd.DataFrame(results_data,columns = ['True','Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True  Predicted\n",
       "0     1        0.0\n",
       "1     1        1.0\n",
       "2     1        1.0\n",
       "3     1        1.0\n",
       "4     1        1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY =  0.8\n"
     ]
    }
   ],
   "source": [
    "print (\"ACCURACY = \",(y_pred== y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
