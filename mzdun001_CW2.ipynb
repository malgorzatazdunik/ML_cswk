{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hours\t0.50  |\t0.75  |\t1.00  |\t1.25  |\t1.50  |\t 1.75  |   1.75  |  2.00  | \t2.25 |\t2.50 |\t2.75 |\t3.00 |\t3.25 |\t3.50 |\t4.00 |\t4.25 |\t4.50 |\t4.75 |\t5.00 |\t5.50 |\n",
    "<br>\n",
    "Pass\t0 |\t0 |\t0 |\t0 |\t0 |\t0 |\t1 |\t0 |\t1 |\t0 |\t1 |0 |1 |\t0 |\t1 |\t1 |\t1 |\t1 |\t1 |\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to do: 1. Link tutorials. 2. comments, doc strings. 3. report 4. questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac <br>\n",
    "https://predictiveprogrammer.com/machine-learning-from-scratch-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.asarray ([[0.50], [0.75], [1.00], [1.25], [1.50], [1.75], [3.00], [4.75], [1.75], [3.25], [5.00], [2.00], [2.25], [2.50], [2.75], [3.50], [4.00], [4.25], [4.50], [5.50]]) \n",
    "y = np.asarray([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,0,1,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:15], y[:15], X[15:], y[15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis. <br>\n",
    "A function takes inputs and returns outputs. To generate probabilities, logistic regression uses a function that gives outputs between 0 and 1 for all values of X. There are many functions that meet this description, but the used in this case is the logistic function. From here we will refer to it as sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros([1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z =np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do: insteads of theta,bias, do theta[0], theta[1] ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def h_lin( X, theta):\n",
    "#     'X should be a data vector, theta the parameters'\n",
    "#     return theta[0] + (theta[1]*X)\n",
    "#1 / (1 + np.exp(-X))\n",
    "def sigmoid(X,theta,bias):\n",
    "    z = np.dot(X, theta) + bias\n",
    "    #print(z)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    #return 1 / (1+ np.exp(theta*X))\n",
    "\n",
    "\n",
    "theta = np.random.rand(X.shape[1], 1)\n",
    "bias = np.zeros((1,))\n",
    "#theta = np.zeros(X.shape[1])\n",
    "#theta = np.zeros([1,])\n",
    "h = sigmoid(X,theta,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44971673]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55597893],\n",
       "       [0.58353149],\n",
       "       [0.61057188],\n",
       "       [0.63694892],\n",
       "       [0.66252684],\n",
       "       [0.68718773],\n",
       "       [0.79399066],\n",
       "       [0.89436786],\n",
       "       [0.68718773],\n",
       "       [0.81177412],\n",
       "       [0.90452829],\n",
       "       [0.71083306],\n",
       "       [0.73338451],\n",
       "       [0.75478394],\n",
       "       [0.77499274],\n",
       "       [0.82835431],\n",
       "       [0.85801095],\n",
       "       [0.87116469],\n",
       "       [0.88326566],\n",
       "       [0.92225891]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function <br>\n",
    "Functions have parameters/weights (represented by theta in our notation) and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function, defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: understand loss function and fix it possibly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossValue = 0 \n",
    "#     for i in range(len(y)):\n",
    "#         lossValue += 0.5 * (h[i] - y[i])**2\n",
    "#     return lossValue\n",
    "\n",
    "\n",
    "\n",
    "def loss_(h, y):\n",
    "#     lossValue = 0\n",
    "#     for i in range(len(y)):\n",
    "#         lossValue += (-y[i] * np.log(h[i]) - (1 - y[i]) * np.log(1 - h[i]))\n",
    "    # entropy when true class is positive\n",
    "#         pos_log = y * np.log(h + 1e-15)\n",
    "#         # entropy when true class is negative\n",
    "#         neg_log = (1 - y) * np.log((1 - h) + 1e-15)\n",
    "\n",
    "#         l = -np.mean(pos_log + neg_log)\n",
    "#         return l\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "#    return lossValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145605310824456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_(h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent <br>\n",
    "Our goal is to minimize the loss function and the way we have to achive it is by increasing/decreasing the weights, i.e. fitting them. The question is, how do we know what parameters should be biggers and what parameters should be smallers? The answer is given by the derivative of the loss function with respect to each weight. It tells us how loss would change if we modified the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.dot(X.T, (h - y)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.27416473,  2.27416473,  2.27416473,  2.27416473,  2.27416473,\n",
       "         2.27416473, -0.51333527,  2.27416473, -0.51333527,  2.27416473,\n",
       "        -0.51333527,  2.27416473, -0.51333527,  2.27416473, -0.51333527,\n",
       "        -0.51333527, -0.51333527, -0.51333527, -0.51333527, -0.51333527]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44971673]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: understand calculating gradient -> shouldn't be the same as linear reg? see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(X,y,alpha,treshold):\n",
    "    loss_list = []\n",
    "\n",
    "    #theta = np.zeros([X.shape[1],])\n",
    "    weights = np.random.rand(X.shape[1])\n",
    "    bias = np.zeros((1,))\n",
    "    #theta = old_theta.copy()\n",
    "\n",
    "   # h = sigmoid(X,theta)\n",
    "    loss = loss_(sigmoid(X,weights,bias),y)\n",
    "    loss_update = 0\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    \n",
    "    while(abs(loss-loss_update)> treshold):\n",
    "    #while(index<50):\n",
    "            \n",
    "            h = sigmoid(X,weights,bias)\n",
    "            \n",
    "            gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "            #print(gradient)\n",
    "            weights -= alpha * gradient\n",
    "            #print('theta update', theta)\n",
    "            bias -= alpha * np.mean(h-y)\n",
    "            \n",
    "            h = sigmoid(X,weights,bias)\n",
    "            \n",
    "            loss = loss_update\n",
    "            loss_update=loss_(h,y)\n",
    "            loss_list.append(loss_update)\n",
    "            #print([loss,loss_update])\n",
    "            \n",
    "            index+=1\n",
    "\n",
    "\n",
    "    print('GD stopped at loss %s, with coefficients: %s' % (loss,[weights,bias]), 'index', index)\n",
    "    return [weights,bias],loss_list\n",
    "    \n",
    "    \n",
    "   # return (probabilities >= treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD stopped at loss 0.49841135862158853, with coefficients: [array([0.9460439]), array([-2.54278989])] index 72\n"
     ]
    }
   ],
   "source": [
    "theta, loss = LogisticRegression(X,y,0.9, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9460439]), array([-2.54278989])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6200302375563302,\n",
       " 0.6116115984783483,\n",
       " 0.6034008124581948,\n",
       " 0.5963678546695084,\n",
       " 0.5891617719346391,\n",
       " 0.5831395434698098,\n",
       " 0.5767523008848265,\n",
       " 0.5714791037449966,\n",
       " 0.5658784267321744,\n",
       " 0.5612257919764571,\n",
       " 0.5564243674189758,\n",
       " 0.5523533859610262,\n",
       " 0.5483222828097267,\n",
       " 0.5448158512414986,\n",
       " 0.5414672903222744,\n",
       " 0.5384857067753948,\n",
       " 0.5357006969217671,\n",
       " 0.5331760830024102,\n",
       " 0.5308376504096367,\n",
       " 0.5286914057791019,\n",
       " 0.526703525991454,\n",
       " 0.5248641994767084,\n",
       " 0.5231555071950746,\n",
       " 0.5215664236386449,\n",
       " 0.5200856277747172,\n",
       " 0.5187040698140419,\n",
       " 0.5174135826884214,\n",
       " 0.5162070189727742,\n",
       " 0.5150779901136147,\n",
       " 0.5140207246429714,\n",
       " 0.5130299995535961,\n",
       " 0.512101038559882,\n",
       " 0.5112294763116284,\n",
       " 0.5104113000026811,\n",
       " 0.5096428216581509,\n",
       " 0.5089206419252037,\n",
       " 0.5082416266365513,\n",
       " 0.5076028813730173,\n",
       " 0.5070017316231896,\n",
       " 0.5064357033102544,\n",
       " 0.5059025061815797,\n",
       " 0.5054000182386498,\n",
       " 0.5049262719312194,\n",
       " 0.5044794414549075,\n",
       " 0.504057831301901,\n",
       " 0.5036598657933832,\n",
       " 0.5032840795751443,\n",
       " 0.5029291089390313,\n",
       " 0.5025936839127252,\n",
       " 0.5022766210316846,\n",
       " 0.5019768167344638,\n",
       " 0.5016932413190911,\n",
       " 0.501424933409591,\n",
       " 0.5011709948843487,\n",
       " 0.5009305862242229,\n",
       " 0.500702922241855,\n",
       " 0.5004872681578008,\n",
       " 0.5002829359923309,\n",
       " 0.5000892812448772,\n",
       " 0.4999056998357788,\n",
       " 0.499731625287441,\n",
       " 0.4995665261241894,\n",
       " 0.49940990347207237,\n",
       " 0.4992612888416147,\n",
       " 0.49912024207811045,\n",
       " 0.49898634946546155,\n",
       " 0.49885922197085053,\n",
       " 0.4987384936186885,\n",
       " 0.49862381998331423,\n",
       " 0.4985148767908658,\n",
       " 0.49841135862158853,\n",
       " 0.49831297770460925]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigmoid(X, theta[0],theta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11207013, 0.13785073, 0.1684369 , 0.20420211, 0.24532116,\n",
       "       0.29168529, 0.57330339, 0.87554677, 0.29168529, 0.62991514,\n",
       "       0.89911499, 0.34283134, 0.3979076 , 0.45569646, 0.51470347,\n",
       "       0.68316683, 0.77580512, 0.81425467, 0.8474071 , 0.93465225])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "plt.plot(y - 0.1,'g*')\n",
    "plt.plot(y_pred,'b.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred== y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
